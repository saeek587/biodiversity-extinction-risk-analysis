# ============================================
# RANDOM FOREST CLASSIFIER
# Conservation Status Prediction
# Author: Saee Kurhade
# BSc Statistics Project | Fergusson College
# ============================================

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (confusion_matrix, classification_report, 
                             accuracy_score, precision_recall_fscore_support)
from sklearn.preprocessing import OrdinalEncoder

# Set display options
pd.set_option('display.max_columns', None)
plt.style.use('seaborn-v0_8')

# ==========================================
# DATA LOADING AND PREPROCESSING
# ==========================================

# Load dataset (update file path as needed)
# dataset = pd.read_csv("factored_table.csv")
# For demonstration, assuming dataset is loaded

print("=" * 60)
print("CONSERVATION STATUS PREDICTION USING RANDOM FOREST")
print("=" * 60)

# Standardize conservation status labels
# LR/* categories were older IUCN classifications
replacement_dict = {
    "LR/lc": "LC",    # Lower Risk/Least Concern → Least Concern
    "LR/cd": "LC",    # Lower Risk/Conservation Dependent → Least Concern
    "LR/nt": "NT",    # Lower Risk/Near Threatened → Near Threatened
    "CR(PE)": "CR",   # Critically Endangered (Possibly Extinct) → CR
    "CR(PEW)": "CR",  # Critically Endangered (Possibly Extinct in Wild) → CR
    "EW": "EX",       # Extinct in Wild → Extinct (for simplification)
    "NR": "DD"        # Not Recognized → Data Deficient
}

ds2 = dataset.replace(replacement_dict)

# Display original data structure
print(f"\nDataset Shape: {ds2.shape}")
print(f"Columns: {ds2.columns.tolist()}")

# Drop non-feature columns
ds2 = ds2.drop(columns=["Scientific name", "Common name"], errors='ignore')

# Check class distribution
print("\nClass Distribution - IUCN Red List 2021:")
print(ds2["IUCN Red List 2021"].value_counts())

print("\nClass Distribution - IUCN Red List 2022:")
print(ds2["IUCN Red List 2022"].value_counts())

# ==========================================
# ONE-HOT ENCODING
# ==========================================
# Random Forest with CART requires one-hot encoding for categorical features

# One-hot encode taxonomy (Feature 1)
ds4 = pd.get_dummies(ds2.iloc[:, 0], prefix='Taxonomy')

# One-hot encode previous year conservation status (Feature 2)
ds5 = pd.get_dummies(ds2.iloc[:, 1], prefix='Previous_Status')

# Combine encoded features
ds3 = pd.concat([ds4, ds5], axis=1)

print(f"\nFeatures after One-Hot Encoding: {ds3.shape[1]} features")
print(f"Sample features: {ds3.columns[:10].tolist()}")

# ==========================================
# FEATURE-TARGET SEPARATION
# ==========================================
# Target: Current year conservation status
y = ds2["IUCN Red List 2022"]

# Features: Taxonomy + Previous year status (one-hot encoded)
X = ds3

print(f"\nFeature matrix shape: {X.shape}")
print(f"Target vector shape: {y.shape}")
print(f"Target classes: {y.unique()}")

# ==========================================
# TRAIN-TEST SPLIT
# ==========================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.25,      # 75% train, 25% test
    random_state=42,     # For reproducibility
    stratify=y           # Maintain class distribution
)

print(f"\nTraining set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")

# ==========================================
# RANDOM FOREST CLASSIFIER
# ==========================================
print("\n" + "="*60)
print("TRAINING RANDOM FOREST CLASSIFIER")
print("="*60)

# Initialize Random Forest
# n_estimators=100: Use 100 decision trees
# criterion='gini': Use Gini index for splits
# random_state=42: For reproducibility
classifier = RandomForestClassifier(
    n_estimators=100, 
    criterion='gini', 
    random_state=42,
    max_depth=None,        # Trees grow until pure leaves
    min_samples_split=2,   # Minimum samples to split node
    min_samples_leaf=1,    # Minimum samples in leaf
    n_jobs=-1              # Use all CPU cores
)

# Train the model
classifier.fit(X_train, y_train)

print("✓ Model training complete!")

# ==========================================
# PREDICTIONS
# ==========================================
# Predict on test set
y_pred = classifier.predict(X_test)

# Predict on training set (to check overfitting)
y_train_pred = classifier.predict(X_train)

# ==========================================
# MODEL EVALUATION
# ==========================================
print("\n" + "="*60)
print("MODEL PERFORMANCE METRICS")
print("="*60)

# Accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_pred)

print(f"\nAccuracy on Training Set: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
print(f"Accuracy on Test Set: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

if train_accuracy - test_accuracy > 0.1:
    print("⚠️  Warning: Potential overfitting detected")
else:
    print("✓ No significant overfitting")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

print("\n--- Confusion Matrix ---")
print(cm)

# Visualize confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('Confusion Matrix - Random Forest Classifier')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.savefig('confusion_matrix_rf.png', dpi=300)
print("✓ Confusion matrix saved as 'confusion_matrix_rf.png'")

# Classification Report
print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

# ==========================================
# FEATURE IMPORTANCE
# ==========================================
print("\n" + "="*60)
print("FEATURE IMPORTANCE ANALYSIS")
print("="*60)

# Extract feature importances
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': classifier.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nTop 15 Most Important Features:")
print(feature_importance.head(15).to_string(index=False))

# Visualize top 10 features
plt.figure(figsize=(10, 6))
top_features = feature_importance.head(10)
plt.barh(range(len(top_features)), top_features['Importance'], color='steelblue')
plt.yticks(range(len(top_features)), top_features['Feature'])
plt.xlabel('Feature Importance (Gini Index)')
plt.title('Top 10 Most Important Features - Random Forest')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('feature_importance_rf.png', dpi=300)
print("✓ Feature importance plot saved as 'feature_importance_rf.png'")

# ==========================================
# DETAILED METRICS BY CLASS
# ==========================================
print("\n" + "="*60)
print("DETAILED METRICS BY CONSERVATION STATUS")
print("="*60)

precision, recall, f1, support = precision_recall_fscore_support(
    y_test, y_pred, average=None, labels=np.unique(y)
)

metrics_df = pd.DataFrame({
    'Status': np.unique(y),
    'Precision': precision,
    'Recall': recall,
    'F1-Score': f1,
    'Support': support
})

print("\n", metrics_df.to_string(index=False))

# ==========================================
# INTERPRETATION
# ==========================================
print("\n" + "="*60)
print("INTERPRETATION")
print("="*60)

print(f"""
Model Performance Summary:
- Overall Accuracy: {test_accuracy*100:.2f}%
- The model correctly predicts conservation status {test_accuracy*100:.2f}% of the time
- Previous year's status is a strong predictor, but not the only factor
- Other factors affecting conservation status:
  → Climate change
  → Habitat loss and degradation
  → Environmental contamination
  → Human activities (poaching, pollution, urbanization)
  → Population dynamics
  → Geographic range changes

Limitations:
- Model only uses taxonomy and previous status
- Does not incorporate biological/ecological factors
- Class imbalance may affect predictions for rare statuses (EX, EW)
- 52% accuracy indicates room for improvement with additional features

Recommendations:
- Incorporate habitat data, population size, geographic range
- Include climate variables and human impact indices
- Use time-series features (multi-year trends)
- Address class imbalance with SMOTE or class weights
""")

# ==========================================
# EXPORT RESULTS
# ==========================================
# Save predictions
results = pd.DataFrame({
    'True_Status': y_test,
    'Predicted_Status': y_pred,
    'Correct': y_test == y_pred
})
results.to_csv('rf_predictions.csv', index=False)

# Save feature importance
feature_importance.to_csv('feature_importance.csv', index=False)

# Save metrics
metrics_df.to_csv('classification_metrics.csv', index=False)

print("\n" + "="*60)
print("✓ Analysis complete! Results exported to CSV files.")
print("="*60)

# ==========================================
# EXAMPLE PREDICTION
# ==========================================
print("\n--- Example Prediction ---")
print("If a species from Mammalia taxonomy was 'VU' in 2021,")
print("what will its status be in 2022?")

# Create example input (would need actual one-hot encoded format)
# This is conceptual - actual implementation would require proper encoding
print("\n[Run model.predict() with properly encoded input features]")
